{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6soMuIUm88Uy"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.2\n",
    "!pip install peft==0.11.1\n",
    "!pip install accelerate==0.30.1\n",
    "!pip install bitsandbytes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "DATA_PATH = \"/content/humor_3cat.jsonl\"\n",
    "OUTPUT_DIR = \"/content/humor_phi3_lora\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    example[\"text\"] = (\n",
    "        f\"<|user|>\\n{example['instruction']}\\n\"\n",
    "        f\"<|assistant|>\\n{example['output']}\"\n",
    "    )\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1.5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=20,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Finetuning complete!\")\n"
   ],
   "metadata": {
    "id": "l0EgLzE8-PII",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0fabdebbbbe64e8e8183c27063b1db15",
      "036cdffa508f4974b42d43ef9380eefd",
      "9e7ed2baccb14f9bb8d5676d2d245228",
      "84756f61484c4e248a7a72d3d76066ca",
      "5d00b4d124f046f2ba6c61a589700d0b",
      "4f787f8fcf7a470ebc467f8e988ba034",
      "f3f0d39392a64c47bab9114f21a383d6",
      "02362d4322fb4123a1523effbecc7594",
      "198a64ce1a074948abf2a2c87601405b",
      "65d696d6346e4f10a8d95145ae05d9ed",
      "478e2ce2d7004d2b88104d0c817c3d42",
      "9f643bd147484e069d2b673071b19114",
      "07d0a4c23e494c5ba32123a2a47b19ab",
      "544d99ed61cb4613b435c89c855f7ac5",
      "e75ee280e4704ce7b53b41999018bc85",
      "6dcd6da908254edea198aa3e0bda2ca2",
      "5eb7acda63c8496582dac92ef6875d9f",
      "5f66d39a7c734e11978c073d7801b89c",
      "72f3ec27047b4b22a6d0f3fdaec5c4bd",
      "38b381a4e96043cbb9bb004cdbcf54bc",
      "b6584f214b9d4da28041ca57c592b245",
      "c3addffc0e2540a2a3dc45df05b5f396",
      "95dd14259ddd4b268ea048ad8cb69e66",
      "8f1be1154a374e009624477c6d37a09c",
      "3196997605704377a064575d579adba6",
      "443d9f659de2472d8b989265950da290",
      "5855b63d8e57429084861b78ca647381",
      "c79c98b5fc0146d5906a7febfb8af606",
      "05c7902165b34c18a85fd672e9e2a842",
      "247d123ec31c44d2a19bccf46f31b2b0",
      "48e8d49e61ab41bfbb8025c8bcb2f6d3",
      "3ed57549a23a4f1789b315a910cdfae1",
      "3bd30e4b44c94e37836cbfeee26b79cc"
     ]
    },
    "outputId": "9f626568-0049-42e5-ceb6-adf32c357a2b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fabdebbbbe64e8e8183c27063b1db15"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f643bd147484e069d2b673071b19114"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95dd14259ddd4b268ea048ad8cb69e66"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='843' max='843' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [843/843 1:21:37, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.910700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.812800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.802500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.740900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.792900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.827900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.902500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.999000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finetuning complete!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "LORA_PATH = \"/content/humor_phi3_lora\"\n",
    "OUT_DIR    = \"/content/phi3_humor_merged\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(model, LORA_PATH)\n",
    "\n",
    "print(\"Merging LoRA...\")\n",
    "model = model.merge_and_unload()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"Saving merged model...\")\n",
    "model.save_pretrained(OUT_DIR, safe_serialization=True)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(\"MERGE COMPLETE!\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ioE1O-Jg8dd7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "92945a4895db4009a4891b7cde6becd2",
      "4836183b2e72443fb6da068c55f2ae9b",
      "2441037076da4535a579d3cd11916fd7",
      "47b0d4fb069f46ee8da2c27ef07f51f7",
      "0542d9ddb9f9499792fc4e8d157e5217",
      "79ccdd889edb48e395e89c30052dc190",
      "5c4b4806846046c5aaeae1d56ef72a48",
      "9596d9c620c046179521cdd12d2a124d",
      "ac7731afc25f4d3b9476769d1ed8d2fd",
      "73d0c416ac35490cb71b17a4efc2fe4a",
      "34059aa0b14d4af6aafb57efb1ac0708"
     ]
    },
    "outputId": "6b559219-16dc-4a77-b0e5-c4b16670c8f9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92945a4895db4009a4891b7cde6becd2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading LoRA adapter...\n",
      "Merging LoRA...\n",
      "Saving merged model...\n",
      "MERGE COMPLETE!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Chat function for prompting\n",
    "\n",
    "MODEL_DIR = \"/content/drive/MyDrive/phi3_humor_merged\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "def chat(prompt):\n",
    "\n",
    "    full_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=70,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=4,\n",
    "    )\n",
    "\n",
    "\n",
    "    new_tokens = outputs[0][input_len:]\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    print(text)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "BL3QDh3nGCX_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "523d1304392c48fbba6531b1c6ccea1e",
      "c8641425d0f448d59922bd560bcde859",
      "401c9c3f750a4fa086391622319fc49e",
      "9cc5fb54f6574c77bdba8a4905ab29d5",
      "1c0c222d63884e2cbacd2039f7e35278",
      "172506b8f9d64238b0fc4f3831aa6af0",
      "e61aa24441684f1486f759ef76945b70",
      "22fcfb2eabf14f4ab25f2b8255675606",
      "67748b10f68e4b53a0098c26dd1f56f5",
      "fdb86b2c1be54395a4c505ae9008a8da",
      "934fc28d4d2c4538986f61f38c6f1d04"
     ]
    },
    "outputId": "c55cb894-fcb4-4e72-bfe2-fcf7c86666c7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "523d1304392c48fbba6531b1c6ccea1e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ]
  }
 ]
}